<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on Wixb blog</title>
    <link>http://wixb50.github.io/categories/docker/</link>
    <description>Recent content in Docker on Wixb blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>wixb50@gmail.com (Wixb)</managingEditor>
    <webMaster>wixb50@gmail.com (Wixb)</webMaster>
    <copyright>(c) 2015 wixb.All rights reserved.</copyright>
    <lastBuildDate>Sat, 17 Sep 2016 10:38:38 +0800</lastBuildDate>
    <atom:link href="http://wixb50.github.io/categories/docker/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>docker1.12 集群实践方案</title>
      <link>http://wixb50.github.io/2016/09/17/docker1.12-%E9%9B%86%E7%BE%A4%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%A1%88/</link>
      <pubDate>Sat, 17 Sep 2016 10:38:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2016/09/17/docker1.12-%E9%9B%86%E7%BE%A4%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%A1%88/</guid>
      <description>

&lt;!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#前言&#34;&gt;&lt;a href=&#34;#&#34;&gt;前言&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#docker私有仓库&#34;&gt;docker私有仓库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#docker-swarm&#34;&gt;Docker swarm&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#docker-service&#34;&gt;docker service&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#add-on&#34;&gt;add on&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#consulregistrator服务注册&#34;&gt;consul、registrator服务注册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graylog-日志收集&#34;&gt;graylog 日志收集&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#参考&#34;&gt;参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;

&lt;h1 id=&#34;前言:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;&lt;a href=&#34;#&#34;&gt;前言&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Docker 1.12将原先独立的项目&lt;code&gt;docker swarm&lt;/code&gt;已经集成进自带的&lt;code&gt;docker engine&lt;/code&gt;，并为集群方案提供了整一套的跨主机集群、灵活调度、高可用性的方案，同时引入service的概念，增加了服务创建的简易性、灵活性，还拥有服务注册、服务发现、服务自动负载均衡等特性。&lt;/p&gt;

&lt;p&gt;在本docker集群方案中，先介绍私有仓库的概念，作为集群服务部署的基础；然后利用docker提供的官方的集群方案swarm搭建集群；最后补充一些其他方案满足整集群的监控，日志收集等方面。&lt;/p&gt;

&lt;h1 id=&#34;docker私有仓库:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;docker私有仓库&lt;/h1&gt;

&lt;p&gt;仓库（Repository）是集中存放镜像文件的场所。仓库分为公开仓库（Public）和私有仓库（Private）两种形式。&lt;/p&gt;

&lt;p&gt;最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。&lt;/p&gt;

&lt;p&gt;当需要加快service的更新速度的时候，就需要在局域网内拥有自建的私有仓库，当用户创建了自己的镜像之后就可以使用 push 命令将它上传到私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。&lt;/p&gt;

&lt;h1 id=&#34;docker-swarm:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;Docker swarm&lt;/h1&gt;

&lt;p&gt;在docker1.12版本中不再试独立的项目，而集成在docker engine中，对于创建和管理集群都是非常方便的。&lt;/p&gt;

&lt;p&gt;初始化集群master节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker swarm init
Swarm initialized: current node (0khsh2muxte9lkh584btlhgkv) is now a manager.

To add a worker to this swarm, run the following command:
    docker swarm join \
    --token SWMTKN-1-3re5jcwzt0yjx5h5gtxgaspgib0gxrs75peyiwufbk9bzr4nmh-f496phvvqbtekc21lulndjnfy \
    192.168.65.2:2377

To add a manager to this swarm, run the following command:
    docker swarm join \
    --token SWMTKN-1-3re5jcwzt0yjx5h5gtxgaspgib0gxrs75peyiwufbk9bzr4nmh-5beo6o66ep6p9yhqh7m2f0h8y \
    192.168.65.2:2377
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在同局域网中worker节点执行上述提供的加入集群命令即可。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;docker swarm集群最小只需一个master节点即可，同时也可以多个manager、多个worker节点；所以为了保证集群的高可用性，可创建多几个manager节点。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;docker-service:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;docker service&lt;/h2&gt;

&lt;p&gt;docker将所有部署的应用都抽象为service，在新建集群之后需要在集群中部署服务，这时候就要利用service命令了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;创建service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service create --name gateway --publish 80:80 --replicas 2 nginx:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.在创建service之后，master会自动下发任务给集群节点，并生成指定数量的服务容器replicas；2.同时由于swarm采用的是overlay(跨主机网络)，在swarm内部会自动将该服务name注册到内部的注册中心；3.如该服务是提供给内部系统调用的话，只需要访问其service name即可自动转发到相应服务，这样就避免了难记又易变得ip地址。4.并且由于service是可能部署多个replicas的，所以swarm也能达到一定的负载均衡的作用。5.我们就只需要对提供给外部网络访问的服务暴露指定的端口&lt;code&gt;--publish 80:80&lt;/code&gt;就可以了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;scale service数量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker service scale gateway=2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时如果由于异常容器退出的话，swarm自带的 &lt;strong&gt;安全检查&lt;/strong&gt; 会删除原来的失败的容器，并重新启动一个新的，保证服务的replicas数量。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;service滚动更新&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当我们代码有更新需要重新部署的时候，重新构建镜像push到仓库(不同tag版本命名)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service update --image new-image-name gateway
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;service更新的时候如果image不一样，则service会自动根据新的image拉取镜像，进行滚动更新。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;删除service
&lt;code&gt;
$ docker service rm gateway
&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;add-on:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;add on&lt;/h1&gt;

&lt;h2 id=&#34;consul-registrator服务注册:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;consul、registrator服务注册&lt;/h2&gt;

&lt;p&gt;由于swarm内部的服务注册和发现机制是内部的，不对外开放的，而且不是通过service启动的容器，swarm是发现不了的，所以为了监控整个集群的所有容器，我们可以自己搭建一套。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;consul注册中心&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;consul用户保存集群中注册的所有容器，提供一个信息存储中心，只需要搭建一个即可(也可部署集群)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -h node \
   --name=consul \
   -p 8500:8500 \
   -p 8600:53/udp \
   progrium/consul:latest \
   -server \
   -bootstrap \
   -advertise $DOCKER_IP \
   -log-level debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过浏览器能够访问$DOCKER_IP:8500，你将在控制面板上看到Consul中已经注册的所有服务。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;registrator注册组件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该组件Registrator配置好相应的环境变量并将这个容器注册到Consul上。它会监控本机的docker进程，如果容器开启会自动注册到consul，关闭则自动移除。所以需要在集群的每台主机上配置安装下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d \
   --name=registrator \
   --net=host \
   -v /var/run/docker.sock:/tmp/docker.sock \
   gliderlabs/registrator:latest \
   consul://$DOCKER_IP:8500
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;graylog-日志收集:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;graylog 日志收集&lt;/h2&gt;

&lt;p&gt;由于服务都是集群部署，上线之后直接使用&lt;code&gt;docker logs&lt;/code&gt;命令查看容器日志是不合理的，同时问题排查也相当困难。&lt;/p&gt;

&lt;p&gt;所以搭建一个统一的日志收集器用以收集容器所有日志。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name graylog -p 9000:9000 -p 12201:12201/udp graylog2/allinone:latest
// 在需要收集日志的容器或者service启动命令中加入(docker官方推荐)
--log-driver=gelf --log-opt gelf-address=udp://192.168.0.42:12201
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样对于直接&lt;code&gt;stdout&lt;/code&gt;的日志将都会被发送到graylog中。至于graylog有什么强大功能就需要自己去发掘了。&lt;/p&gt;

&lt;h1 id=&#34;参考:a2fc1d5a9ac95ddb7941c4817733157b&#34;&gt;参考&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://dockone.io/article/272&#34;&gt;http://dockone.io/article/272&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集群项目
   * Docker compose、Docker machine、Docker swarm
   * Kubernetes&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker入门</title>
      <link>http://wixb50.github.io/2016/06/17/docker%E5%85%A5%E9%97%A8/</link>
      <pubDate>Fri, 17 Jun 2016 19:38:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2016/06/17/docker%E5%85%A5%E9%97%A8/</guid>
      <description>

&lt;!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:0 orderedList:0 --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#前言&#34;&gt;&lt;a href=&#34;#&#34;&gt;前言&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#基本概念&#34;&gt;基本概念&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#镜像&#34;&gt;镜像&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#容器&#34;&gt;容器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#仓库&#34;&gt;仓库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#安装&#34;&gt;安装&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#常用命令&#34;&gt;常用命令&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#镜像命令&#34;&gt;镜像命令&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#容器命令&#34;&gt;容器命令&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#docker进阶&#34;&gt;Docker进阶&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#数据卷&#34;&gt;数据卷&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#主机容器数据共享&#34;&gt;主机容器数据共享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#数据卷容器&#34;&gt;数据卷容器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dockerfile&#34;&gt;Dockerfile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;

&lt;h1 id=&#34;前言&#34;&gt;&lt;a href=&#34;#&#34;&gt;前言&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;虚拟化&lt;/strong&gt;，是指通过虚拟化技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt; 是个开源项目，它彻底释放了虚拟化的威力，极大提高了应用的运行效率，降低了云计算资源供应的成本，同时让应用的部署、测试和分发都变得前所未有的高效和轻松。Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。&lt;/p&gt;

&lt;p&gt;如果把虚拟化比做OS，则Docker是OS上的再一层抽象，它运行的容器可以看作一个进程级别的虚拟机。启动、运行、安装都是只通过一个命令就能解决。对于搭建集群、系统CI和不可变基础设施是非常有利的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker简单原理&lt;/strong&gt; Docker只是管理运行轻量级容器的工具，容器是基于Linux内核技术包括namespace，cgroup，叠加文件系统如AUFS，工具提供运行应用，打包应用，分发应用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker影响&lt;/strong&gt; Docker成为最火热，甚至最具颠覆性的技术之一。相对于传统的云服务提供商(Iaas,Paas,Saas)，提出了一种全新的Caas(容器即服务)，国内比较领先的有DaoCloud、时速云等。Docker提出了“Build once，Run anywhere。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://www.otokaze.cn/wp-content/uploads/2016/09/docker_vs_vm.png&#34; alt=&#34;原理图&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Docker 包括三个基本概念&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;镜像（Image）&lt;/li&gt;
&lt;li&gt;容器（Container）&lt;/li&gt;
&lt;li&gt;仓库（Repository）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;理解了这三个概念，就理解了 Docker 的整个生命周期。&lt;/p&gt;

&lt;h2 id=&#34;镜像&#34;&gt;镜像&lt;/h2&gt;

&lt;p&gt;Docker 镜像（Image）就是一个只读的模板，相当于操作系统(OS)安装的Ghost。&lt;/p&gt;

&lt;p&gt;例如：一个镜像可以包含一个完整的 ubuntu 操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。&lt;/p&gt;

&lt;p&gt;镜像可以用来创建 Docker 容器。&lt;/p&gt;

&lt;p&gt;Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。&lt;/p&gt;

&lt;h2 id=&#34;容器&#34;&gt;容器&lt;/h2&gt;

&lt;p&gt;Docker 利用容器（Container）来运行应用。&lt;/p&gt;

&lt;p&gt;容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。同时容器也可以暴露相应的“端口”和挂载“容器卷”，分别用以网络和存储共享。&lt;/p&gt;

&lt;p&gt;可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。&lt;/p&gt;

&lt;h2 id=&#34;仓库&#34;&gt;仓库&lt;/h2&gt;

&lt;p&gt;仓库（Repository）是集中存放镜像文件的场所。&lt;/p&gt;

&lt;p&gt;仓库分为公开仓库（Public）和私有仓库（Private）两种形式。&lt;/p&gt;

&lt;p&gt;最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。&lt;/p&gt;

&lt;p&gt;国内的公开仓库包括 时速云 、网易云 等，可以提供大陆用户更稳定快速的访问。&lt;/p&gt;

&lt;p&gt;当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。&lt;/p&gt;

&lt;p&gt;*注：Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。&lt;/p&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;Docker 目前只能安装在 64 位平台上，并且要求内核版本不低于 3.10，实际上内核越新越好，过低的内核版本容易造成功能的不稳定。&lt;/p&gt;

&lt;p&gt;快捷安装脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;常用命令&#34;&gt;常用命令&lt;/h1&gt;

&lt;h2 id=&#34;镜像命令&#34;&gt;镜像命令&lt;/h2&gt;

&lt;p&gt;获取镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker pull ubuntu:12.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载过程中，会输出获取镜像的每一层信息。
该命令实际上相当于 $ sudo docker pull registry.hub.docker.com/ubuntu:12.04 命令，即从注册服务器 registry.hub.docker.com 中的 ubuntu 仓库来下载标记为 12.04 的镜像。&lt;/p&gt;

&lt;p&gt;使用 docker images 显示本地已有的镜像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker images
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;移除本地的镜像，可以使用 docker rmi 命令。注意 docker rm 命令是移除容器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker rmi training/sinatra
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;容器命令&#34;&gt;容器命令&lt;/h2&gt;

&lt;p&gt;基于一个images，新建并启动一个容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run ubuntu:14.04 /bin/echo &#39;Hello world&#39;
Hello world
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列出所有启动的容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列出所有的容器(包括未启动的)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker ps -a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据容器id，或者name启动，重启，停止容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker start/restart {containerID|containerName}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多的时候，需要让 Docker在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d ubuntu:14.04 /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot;
77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时容器会在后台运行并不会把输出的结果(STDOUT)打印到宿主机上面(输出结果可以用docker logs 查看)。&lt;/p&gt;

&lt;p&gt;容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p 参数来指定端口映射。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d -p 5000:5000 training/webapp python app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 hostPort:containerPort 格式本地的 5000 端口映射到容器的 5000 端口.&lt;/p&gt;

&lt;h1 id=&#34;docker进阶&#34;&gt;Docker进阶&lt;/h1&gt;

&lt;h2 id=&#34;数据卷&#34;&gt;数据卷&lt;/h2&gt;

&lt;p&gt;用于Docker 内部以及容器之间管理数据共享与存储。&lt;/p&gt;

&lt;h3 id=&#34;主机容器数据共享&#34;&gt;主机容器数据共享&lt;/h3&gt;

&lt;p&gt;挂载一个主机目录作为数据卷,使用 -v 标记也可以指定挂载一个本地主机的目录到容器中去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后对容器相应的目录更改便直接反映在主机中了。&lt;/p&gt;

&lt;h3 id=&#34;数据卷容器&#34;&gt;数据卷容器&lt;/h3&gt;

&lt;p&gt;数据卷容器，其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的。&lt;/p&gt;

&lt;p&gt;首先，创建一个命名的数据卷容器 dbdata：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在其他容器中使用 &amp;ndash;volumes-from 来挂载 dbdata 容器中的数据卷。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d --volumes-from dbdata --name db1 training/postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h2&gt;

&lt;p&gt;docker 采用分层存储驱动，使用 Dockerfile 可以允许用户创建自定义的镜像。&lt;/p&gt;

&lt;p&gt;Dockerfile 分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Base image to use, this must be set as the first line
FROM ubuntu

# Maintainer: docker_user &amp;lt;docker_user at email.com&amp;gt; (@docker_user)
MAINTAINER docker_user docker_user@email.com

# Commands to update the image
RUN echo &amp;quot;deb http://archive.ubuntu.com/ubuntu/ raring main universe&amp;quot; &amp;gt;&amp;gt; /etc/apt/sources.list
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y nginx
RUN echo &amp;quot;\ndaemon off;&amp;quot; &amp;gt;&amp;gt; /etc/nginx/nginx.conf

# Commands when creating a new container
CMD /usr/sbin/nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，一开始必须指明所基于的镜像名称，接下来推荐说明维护者信息。&lt;/p&gt;

&lt;p&gt;后面则是镜像操作指令，例如 RUN 指令，RUN 指令将对镜像执行跟随的命令。每运行一条 RUN 指令，镜像添加新的一层，并提交。&lt;/p&gt;

&lt;p&gt;最后是 CMD 指令，来指定运行容器时的操作命令。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker集群系列之－Kubernetes对象文件定义</title>
      <link>http://wixb50.github.io/2016/03/13/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Bkubernetes%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6%E5%AE%9A%E4%B9%89/</link>
      <pubDate>Sun, 13 Mar 2016 15:10:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2016/03/13/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Bkubernetes%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6%E5%AE%9A%E4%B9%89/</guid>
      <description>

&lt;h2 id=&#34;说明&#34;&gt;说明&lt;/h2&gt;

&lt;p&gt;主要对kubernetes用户需要定义的Pod,RC和Service的配置文件进行详细说明。&lt;/p&gt;

&lt;h2 id=&#34;pod定义文件详解&#34;&gt;Pod定义文件详解&lt;/h2&gt;

&lt;p&gt;Pod定义文件模板(yaml格式)如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1      # required
kind: Pod           # required
metadata:           # required
  name: string      # required
  namespace: string      # required
  labels:
    - name: string
  annotations:
    - name: string
spec:       # required
  containers:       # required
    - name: string      # required
      image: string     # required
      imagePullPolicy: [Always | Never | IfNotPresent]
      command: [string]
      workingDir: string
      volumeMounts:
        - name: string
          mountPath: string
          readOnly: boolean
      ports:
        - name: string
          containerPort: int
          hostPort: int
          protocol: string
      env:
        - name: string
          value: string
      resources:
        limits:
         cpu: string
         memory: string
  volumes:
    - name: string
      # Either emptyDir for an empty directory
      emptyDir: {}
      # Or hostPath for a pre-existing directory on the host
      hostPath:
        path: string
  restartPolicy: [Always | Never | OnFailure]
  dnsPlicy: [Default | ClusterFirst]        # required
  nodeSelector: object
  imagePullSercret: object
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对Pod各属性详细说明表如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;属 性 名 称&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 类 型&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;是 否 必 选&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;version&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;v1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;kind&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Pod&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;元数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;rc定义文件详解&#34;&gt;RC定义文件详解&lt;/h2&gt;

&lt;p&gt;RC(ReplicationController)定义文件模板(yaml格式)如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1      # required
kind: ReplicationController           # required
metadata:           # required
  name: string      # required
  namespace: string      # required
  labels:
    - name: string
  annotations:
    - name: string
spec:       # required
  replicas: number      # required
  selector: []      # required
  template: object      # required
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;service定义文件详解&#34;&gt;Service定义文件详解&lt;/h2&gt;

&lt;p&gt;Service定义文件模板(yaml格式)如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1      # required
kind: Service           # required
metadata:           # required
  name: string      # required
  namespace: string      # required
  labels:
    - name: string
  annotations:
    - name: string
spec:       # required
  selector: []      # required
  type: string      #required
  clusterIP: string
  sessionAffinity: string
  ports:
    - name: string
      port: int
      targetPort: int
      protocol: string
  status:
    loadBalancer:
      ingress:
        ip: string
        hostname: string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对Service各属性详细说明表如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;属 性 名 称&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 类 型&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;是 否 必 选&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;version&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;v1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;kind&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Pod&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;元数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.name&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Service名称，符合RFC1035规范&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.namespace&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;命名空间，不指定时系统使用名为&amp;raquo;default&amp;raquo;的命名空间&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.labels[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;自定义标签属性列表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.annotation[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;自定义注解属性列表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;详细描述&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.selector[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Label Selector配置，将选择具有指定label标签的Pod作为管理范围&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.type&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;string&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Service类型，指定Service访问方式，默认为ClusterIP。&lt;br&gt;ClusterIP: 虚拟的服务IP地址，该地址用户kubernetes集群内部Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发;&lt;br&gt;NodePort: 使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务;&lt;br&gt;LoadBalancer: 使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，并同时定义nodePort和clusterIP.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.clusterIP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;虚拟服务IP地址，当type=ClusterIP时，如果不指定，则系统自动分配;当type=LoadBalancer时，则需要指定。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.sessionAffinity&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是否支持Session，可选值为ClientIP，默认为空。&lt;br&gt;ClientIP: 表示同一个客户端(根据客户端的IP地址决定)的访问请求都转发到同一个后端Pod。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Service需要暴露的端口号列表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].name&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;端口名称&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].port&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;int&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;服务监听的端口号&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].targetPort&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;int&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;需要转发到后端Pod的端口号&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].protocol&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;int&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;端口协议，支持TCP和UDP，默认TCP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;当spec.type=LoadBalancer时，设置外部负载均衡器的地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer.ingress&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer.ingress.ip&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;string&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器的IP地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer.ingress.hostname&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;string&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器的主机名&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;附录&#34;&gt;附录&lt;/h2&gt;

&lt;p&gt;根据yaml创建：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f &amp;lt;filename.yaml&amp;gt; [--validate[=true]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据yaml删除：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete -f &amp;lt;filename.yaml&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Docker集群系列之－ESXi5.5上搭建基于CoreOS的kubernetes集群</title>
      <link>http://wixb50.github.io/2015/12/30/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8Ecoreos%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Wed, 30 Dec 2015 19:10:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/12/30/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8Ecoreos%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/</guid>
      <description>

&lt;h1 id=&#34;目录:b041d08cd2cacb464d298a81037e9efc&#34;&gt;目录&lt;/h1&gt;

&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coreos-on-vmware&#34;&gt;CoreOS on VMware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloud-config-for-master-node&#34;&gt;Cloud-Config for master node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloud-config-for-minion-node&#34;&gt;Cloud-Config for minion node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#start-the-cluster&#34;&gt;Start the cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#check&#34;&gt;Check&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#enjoy&#34;&gt;Enjoy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;h1 id=&#34;introduction:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Create a Kubernetes Cluster on VMware ESXi with CoreOS.&lt;/p&gt;

&lt;h1 id=&#34;prerequisites:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Prerequisites&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;VMware ESXi

&lt;ul&gt;
&lt;li&gt;(optional) a DRS cluster with VCenter for high-availability host.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DHCP server&lt;/li&gt;
&lt;li&gt;A VMware datastore&lt;/li&gt;
&lt;li&gt;vSphere&lt;/li&gt;
&lt;li&gt;Attention: This requires at least CoreOS version 695.0.0, which includes etcd2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;coreos-on-vmware:b041d08cd2cacb464d298a81037e9efc&#34;&gt;CoreOS on VMware&lt;/h1&gt;

&lt;p&gt;Based on official documentation : &lt;a href=&#34;https://coreos.com/os/docs/latest/booting-on-vmware.html&#34;&gt;https://coreos.com/os/docs/latest/booting-on-vmware.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Download the OVA, on your local computer :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://alpha.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import ova on VMware via the vSphere Client :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in the menu, click &amp;quot;File &amp;gt; Deploy OVF Template...&amp;quot;
in the wizard, specify the location of the OVA downloaded earlier
name your VM
confirm the settings then click &amp;quot;Finish&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a template via vSphere Client :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;right click on the VM and Template &amp;gt; Convert into template
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can create -at least- 2 servers based on this template, do this task but don&amp;rsquo;t start it yet.&lt;/p&gt;

&lt;h1 id=&#34;cloud-config-for-master-node:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Cloud-Config for master node&lt;/h1&gt;

&lt;p&gt;On the VMware datastore, create a directory and initialize config, example :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p &amp;lt;path to datastore&amp;gt;/cloud-config/master/openstack/latest/ 
cd &amp;lt;path to datastore&amp;gt;/cloud-config/master/openstack/latest/
wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/getting-started-guides/coreos/cloud-configs/master.yaml &amp;amp;&amp;amp; mv master.yaml user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t forget to add your ssh_key :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim user_data
users:
  - name: &amp;quot;core&amp;quot;
    groups:
      - &amp;quot;sudo&amp;quot;
      - &amp;quot;docker&amp;quot;
    ssh-authorized-keys:   
      - ssh-rsa AAAA...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the master need a static ip address,so you need to add the ip config to &lt;code&gt;user_data&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim user_data
write-files:
  [...]
  - path: /etc/systemd/network/static.network
    permissions: 0644
    content: |
      [Match]
      Name=ens192  # The network card

      [Network]
      Address=192.1.1.150/24
      Gateway=192.1.1.1
      DNS=10.11.248.114
      DNS=8.8.4.4
  [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you can&amp;rsquo;t get the files on VM,you need download files first,and deploy to your &lt;code&gt;File Server&lt;/code&gt;first.And change the url in the &lt;code&gt;user_data&lt;/code&gt; to your own file position.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Replace `https://github.com/kelseyhightower/setup-network-environment/releases/download/v1.0.0/setup-network-environment` To `&amp;lt;your file url&amp;gt;/setup-network-environment`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-apiserver` To `&amp;lt;your file url&amp;gt;/kube-apiserver`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-controller-manager` To `&amp;lt;your file url&amp;gt;/kube-controller-manager`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-scheduler` To `&amp;lt;your file url&amp;gt;/kube-scheduler`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finaly, replace all &amp;laquo;$private_ipv4&amp;raquo; pattern with the ip of master node. The only way to perform this is to fix a DHCP lease with the MAC address of your master server. This MAC address can be get on vsphere : right click on VM, network adapter. Here, 10.0.0.1 is the master fixed ip address.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;s|$private_ipv4|10.0.0.1|g&#39; user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the limitation with coreOS and VMware : &lt;a href=&#34;https://coreos.com/os/docs/latest/booting-on-vmware.html#cloud-config&#34;&gt;https://coreos.com/os/docs/latest/booting-on-vmware.html#cloud-config&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Last step : create an iso :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd &amp;lt;path to datastore&amp;gt;/cloud-config/
mkisofs -R -V config-2 -o config-master.iso master/
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;cloud-config-for-minion-node:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Cloud-Config for minion node&lt;/h1&gt;

&lt;p&gt;On the VMware datastore, create a directory and initialize config, example :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p &amp;lt;path to datastore&amp;gt;/cloud-config/minion/openstack/latest/
cd &amp;lt;path to datastore&amp;gt;/cloud-config/minion/openstack/latest/
wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/getting-started-guides/coreos/cloud-configs/node.yaml &amp;amp;&amp;amp; mv node.yaml user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t forget to add your ssh_key :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim user_data
users:
  - name: &amp;quot;core&amp;quot;
    groups:
      - &amp;quot;sudo&amp;quot;
      - &amp;quot;docker&amp;quot;
    ssh-authorized-keys:   
      - ssh-rsa AAAA...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you can&amp;rsquo;t get the files on VM,you need download files first,and deploy to your &lt;code&gt;File Server&lt;/code&gt;first.And change the url in the &lt;code&gt;user_data&lt;/code&gt; to your own file position.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Replace `https://github.com/kelseyhightower/setup-network-environment/releases/download/v1.0.0/setup-network-environment` To `&amp;lt;your file url&amp;gt;/setup-network-environment`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy` To `&amp;lt;your file url&amp;gt;/kube-proxy`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kubelet` To `&amp;lt;your file url&amp;gt;/kubelet`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finaly, replace all &amp;laquo;&lt;master-private-ip&gt;&amp;raquo; pattern with the ip of master node. (here 10.0.0.1)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;s|&amp;lt;master-private-ip&amp;gt;|10.0.0.1|g&#39; user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last step : create an iso :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd &amp;lt;path to datastore&amp;gt;/cloud-config/
mkisofs -R -V config-2 -o config-minion.iso minion/
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;start-the-cluster:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Start the cluster&lt;/h1&gt;

&lt;p&gt;On firt VM, mount the config-master.iso with VM properties (CD/DVD reader and &amp;laquo;Datastore ISO file&amp;raquo;), browse to &amp;laquo;&lt;path to datastore&gt;/cloud-config/&amp;laquo;. Don&amp;rsquo;t foget to set &amp;laquo;Connect on Start up&amp;raquo;.&lt;/p&gt;

&lt;p&gt;On second, and all other futher nodes  mount the config-minion.iso.&lt;/p&gt;

&lt;p&gt;Start your servers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;br /&gt;
In order to build the &lt;code&gt;flanneld&lt;/code&gt;,the VMs need to pull the images called &lt;code&gt;quay.io/coreos/flannel&lt;/code&gt;.And if the VMs can&amp;rsquo;t download it,you should get it first,then use the command to load the image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker load &amp;lt; flanneld-file.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ensure all the &lt;code&gt;service&lt;/code&gt; are running.&lt;br /&gt;
&lt;strong&gt;Master:&lt;/strong&gt;&lt;code&gt;docker&lt;/code&gt;、&lt;code&gt;etcd2&lt;/code&gt;、&lt;code&gt;fleet&lt;/code&gt;、&lt;code&gt;flanneld&lt;/code&gt;、&lt;code&gt;setup-network-environment&lt;/code&gt;、&lt;code&gt;kube-apiserver&lt;/code&gt;、&lt;code&gt;kube-controller-manager&lt;/code&gt;、&lt;code&gt;kube-scheduler&lt;/code&gt;.&lt;br /&gt;
&lt;strong&gt;Node:&lt;/strong&gt;&lt;code&gt;docker&lt;/code&gt;、&lt;code&gt;etcd2&lt;/code&gt;、&lt;code&gt;fleet&lt;/code&gt;、&lt;code&gt;flanneld&lt;/code&gt;、&lt;code&gt;setup-network-environment&lt;/code&gt;、&lt;code&gt;kubelet&lt;/code&gt;、&lt;code&gt;kube-proxy&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#you may use these command to start/enable your service:
sudo systemctl daemon-reload
sudo systemctl start &amp;lt;service-name&amp;gt;  #start the service
sudo systemctl enable &amp;lt;service-name&amp;gt;  #Ensure service can boot from the start
sudo systemctl status &amp;lt;service-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;check:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Check&lt;/h1&gt;

&lt;p&gt;Check your cluster heatlh : &lt;a href=&#34;http://10.0.0.1:8080/static/app/#/dashboard/&#34;&gt;http://10.0.0.1:8080/static/app/#/dashboard/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check each server :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh core@10.0.0.1
fleetctl list-machines
# and
journalctl -f
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;enjoy:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Enjoy&lt;/h1&gt;

&lt;p&gt;You may download the kubernetes client tool:&lt;code&gt;kubectl&lt;/code&gt;.Use it manage your cluster.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;get all minion node info.
&lt;code&gt;
kubectl get nodes
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;get all Pods.
&lt;code&gt;
kubectl get pods
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;get all Replication Controllers.
&lt;code&gt;
kubectl get rc
&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;get all Replication Services.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get svc
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;reference:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Reference&lt;/h1&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/xavierbaude/VMware-coreos-multi-nodes-Kubernetes&#34;&gt;VMware-coreos-multi-nodes-Kubernetes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000002886795&#34;&gt;kubernetes 0.18.1 安装 &amp;amp; 部署 &amp;amp; 初试&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services&#34;&gt;Installing Kubernetes Cluster with 3 minions on CentOS 7 to manage pods and services&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://dockerpool.com/article/1422538730&#34;&gt;如何在 CoreOS 集群上搭建 Kubernetes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://qiankunli.github.io/2015/01/29/Kubernetes_installation.html&#34;&gt;在CoreOS集群上搭建Kubernetes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://dockone.io/article/604&#34;&gt;CoreOS集成Kubernetes核心组件Kubelet&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;appendix:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Appendix&lt;/h1&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://gist.github.com/anonymous/553e448c0f8ce9a23120&#34;&gt;source&lt;/a&gt; of &lt;code&gt;master.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://gist.github.com/anonymous/ce88bdc1f6368c0b1589&#34;&gt;source&lt;/a&gt; of &lt;code&gt;node.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker集群系列之－ESXi5.5上搭建CoreOS集群-01</title>
      <link>http://wixb50.github.io/2015/12/15/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BAcoreos%E9%9B%86%E7%BE%A4-01/</link>
      <pubDate>Tue, 15 Dec 2015 18:10:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/12/15/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BAcoreos%E9%9B%86%E7%BE%A4-01/</guid>
      <description>

&lt;h2 id=&#34;目录:8a4718f7d5df59499af8fd92000810ea&#34;&gt;目录&lt;/h2&gt;

&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#null-link&#34;&gt;前言&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#前置条件&#34;&gt;前置条件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#安装coreos虚拟机&#34;&gt;安装CoreOS虚拟机&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-channel&#34;&gt;Choosing a Channel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deploying-with-vmware-vsphere-client-55&#34;&gt;Deploying with VMware vSphere Client 5.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloud-config&#34;&gt;Cloud-Config&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#需要说明的discovery&#34;&gt;需要说明的&lt;code&gt;discovery&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logging-in&#34;&gt;Logging in&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-new-machines&#34;&gt;Adding New Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#一些有用的coreos命令&#34;&gt;一些有用的CoreOS命令&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#参考资料&#34;&gt;参考资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;h2 id=&#34;前言-null-link:8a4718f7d5df59499af8fd92000810ea&#34;&gt;&lt;a href=&#34;chrome://not-a-link&#34;&gt;前言&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;因为大多数环境都是适配于大公司的云平台，但是也是有折中办法的。CoreOS是一个基于Linux 内核的轻量级操作系统，为了计算机集群的基础设施建设而生，专注于自动化，轻松部署，安全，可靠，规模化。作为一个操作系统，CoreOS 提供了在应用容器内部署应用所需要的基础功能环境以及一系列用于服务发现和配置共享的内建工具。而ESXi专为运行虚拟机、最大限度降低配置要求和简化部署而设计。所以我觉得使用ESXi当作IaaS架构，运行CoreOS集群，这样是可行的。话不多少，开始把。&lt;/p&gt;

&lt;h2 id=&#34;前置条件:8a4718f7d5df59499af8fd92000810ea&#34;&gt;前置条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;安装ESXi机器一台：怎么装自己应该知道把，如果因为驱动原因还需要自己定制安装ISO，见Google。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装coreos虚拟机:8a4718f7d5df59499af8fd92000810ea&#34;&gt;安装CoreOS虚拟机&lt;/h2&gt;

&lt;h3 id=&#34;choosing-a-channel:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Choosing a Channel&lt;/h3&gt;

&lt;p&gt;CoreOS is released into alpha, beta, and stable channels. Releases to each channel serve as a release-candidate for the next channel. For example, a bug-free alpha release is promoted bit-for-bit to the beta channel.&lt;/p&gt;

&lt;p&gt;The channel is selected based on the URLs below. Simply replace &lt;code&gt;stable&lt;/code&gt; with &lt;code&gt;alpha&lt;/code&gt; or &lt;code&gt;beta&lt;/code&gt; in the URL. Select 1 of these to download the appropriate image. Read the release notes for specific features and bug fixes in each channel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://stable.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://beta.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://alpha.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;deploying-with-vmware-vsphere-client-5-5:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Deploying with VMware vSphere Client 5.5&lt;/h3&gt;

&lt;p&gt;Use the vSphere Client to deploy the VM as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;in the menu, click “File &amp;gt; Deploy OVF Template…”&lt;/li&gt;
&lt;li&gt;in the wizard, specify the location of the OVA downloaded earlier&lt;/li&gt;
&lt;li&gt;name your VM&lt;/li&gt;
&lt;li&gt;choose “thin provision” for the disk format if you want the disk to grow dynamically&lt;/li&gt;
&lt;li&gt;choose your network settings&lt;/li&gt;
&lt;li&gt;confirm the settings then click “Finish”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NOTE: Unselect “Power on after deployment” so you have a chance to edit VM settings before powering it up for the first time.&lt;/p&gt;

&lt;p&gt;The last step uploads the files to your ESXi datastore and registers your VM. You can now tweak the VM settings, like memory and virtual cores. These instructions were tested to deploy to an ESXi 5.1 host.&lt;/p&gt;

&lt;p&gt;Before powering it on, you will have to create a cloud-config.&lt;/p&gt;

&lt;h2 id=&#34;cloud-config:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Cloud-Config&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/os/docs/latest/cloud-config.html&#34;&gt;Cloud-Config&lt;/a&gt;是CoreOS内比较重要的概念，可以理解为一种配置CoreOS的方式：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Providing Cloud-Config with Config-Drive&lt;/strong&gt;&lt;br /&gt;
Cloud-config can be specified by via &lt;a href=&#34;https://github.com/coreos/coreos-cloudinit/blob/master/Documentation/config-drive.md&#34;&gt;config-drive&lt;/a&gt; with the filesystem label &lt;code&gt;config-2&lt;/code&gt;. This is commonly done through whatever interface allows for attaching CD-ROMs or new drives.&lt;/p&gt;

&lt;p&gt;First create a user_data file using the the &lt;a href=&#34;https://coreos.com/os/docs/latest/cloud-config.html&#34;&gt;cloud-config guide&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config
hostname: core-01  #替换成你的命名的主机名
write_files:
    - path: /etc/systemd/network/static.network
      permissions: 0644  #文件权限,无需改
      content: |
        [Match]
        Name=ens192  #网卡名称,如果你的是别的名称,请改回来

        [Network]
        Address=192.1.1.150/24  #网络配置,同时把下面的IP改掉
        Gateway=192.1.1.1
        DNS=10.11.248.114
        DNS=8.8.4.4
coreos:
    etcd2:
        # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
        discovery: https://discovery.etcd.io/&amp;lt;token&amp;gt;  #这里在后面详细讲
        # multi-region and multi-cloud deployments need to use 192.1.1.150
        advertise-client-urls: http://192.1.1.150:2379
        initial-advertise-peer-urls: http://192.1.1.150:2380
        # listen on both the official ports and the legacy ports
        # legacy ports can be omitted if your application doesn&#39;t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://192.1.1.150:2380,http://192.1.1.150:7001
    fleet:
        public-ip: 192.1.1.150
        metadata: region=europe #metadata,可自定义
    flannel:
        etcd_prefix: /coreos.com/network2
    locksmith:
        endpoint: 192.1.1.150:4001
    update:
        reboot-strategy: etcd-lock
        group: stable
    units:
        - name: etcd2.service #注意是etcd2,第二版哟
          command: start
        - name: fleet.service
          command: start
users:
  - name: &amp;quot;core&amp;quot;  #改成你的用户名,可不是core
    groups:
      - &amp;quot;sudo&amp;quot;
      - &amp;quot;docker&amp;quot;
    ssh-authorized-keys:   
      - ssh-rsa 替换成你的公钥... 
manage_etc_hosts: localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cloud-Config配置信息验证地址&lt;a href=&#34;https://coreos.com/validate/&#34;&gt;https://coreos.com/validate/&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;需要说明的-discovery:8a4718f7d5df59499af8fd92000810ea&#34;&gt;需要说明的&lt;code&gt;discovery&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;　　因为要搭建集群，需要用到服务发现，配置集群的服务发现有两种方式：一种是Static方式，第二种就是Discovery方式了。其中个人不推荐第一种方式，因为每加入一台主机就需要手动配置etcd节点，非常不方便。&lt;br /&gt;
　　第二种Discovery方式是使用远程的服务器辅助服务发现，只需要配置好Discovery的URl就可以自动把新加入的服务器加入集群。其中iscovery服务器可以使用官网提供的，也可以自己搭建(我还没搭建过，这里不介绍了)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl https://discovery.etcd.io/new?size=3  #控制台或者浏览器执行即可,推荐使用size=1,见下面说明
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中有一个&lt;code&gt;size&lt;/code&gt;参数，讲一下我遇到的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;没有使用size参数结果老是启动不了;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;使用了&lt;code&gt;size=3&lt;/code&gt;，结果启动主节点，主节点的etcd2就一直等待从节点加入，结果等我去加入它的时候，已经超时了;&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;size=1&lt;/code&gt;，没有什么要等待了，过一会就自动启动成功了&lt;code&gt;fleetctl list-machines&lt;/code&gt;也能正常显示。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以，我可能不知道网上哪些一下子启动3个节点是怎么做到的，还是有待学习。但是这里我也有自己的解决方法，就是使用&lt;code&gt;size=1&lt;/code&gt;先运行出来一个只有一台主机的集群，果然可以运行。然后使用主节点的&lt;code&gt;&amp;lt;Token&amp;gt;&lt;/code&gt;再去构建其他节点的&lt;code&gt;Cloud-config&lt;/code&gt;，然后运行，结果果然它自己就能加入到第一个节点里面。&lt;/p&gt;

&lt;p&gt;这里我可能投机取巧了点，但是能运行，也能达到效果就行，哈哈，希望不会有什么bug。&lt;/p&gt;

&lt;p&gt;Finally, to create a cloud-config ISO, use the following commands using the user_data file we just created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#wrap up a config named user_data in a config drive image:
mkdir -p /tmp/new-drive/openstack/latest
cp user_data /tmp/new-drive/openstack/latest/user_data
mkisofs -R -V config-2 -o configdrive-01.iso /tmp/new-drive
rm -r /tmp/new-drive

#transform iso file to datastore
#scp configdrive-01.iso root@192.1.1.132:/vmfs/volumes/datastore1/ISO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the config-drive standard was originally an OpenStack feature, which is why you’ll see strings containing openstack. This filepath needs to be retained, although CoreOS supports config-drive on all platforms.&lt;/p&gt;

&lt;p&gt;Note: The $private_ipv4 and $public_ipv4 substitution variables referenced in other documents are not supported on VMware. You can replace all these variables by the (static) IP of the CoreOS server you’re setting up. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;coreos:
  etcd2:
    # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
    discovery: https://discovery.etcd.io/&amp;lt;token&amp;gt;
    # multi-region and multi-cloud deployments need to use $public_ipv4
    advertise-client-urls: http://$public_ipv4:2379
    initial-advertise-peer-urls: http://$private_ipv4:2380
    # listen on both the official ports and the legacy ports
    # legacy ports can be omitted if your application doesn&#39;t depend on them
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;becomes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;coreos:
  etcd2:
    # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
    discovery: https://discovery.etcd.io/&amp;lt;token&amp;gt;
    # multi-region and multi-cloud deployments need to use $public_ipv4
    advertise-client-urls: http://192.168.0.100:2379
    initial-advertise-peer-urls: http://192.168.0.100:2380
    # listen on both the official ports and the legacy ports
    # legacy ports can be omitted if your application doesn&#39;t depend on them
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://192.168.0.100:2380,http://192.168.0.100:7001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Attach the ISO to the VM as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;edit the settings of the CoreOS VM&lt;/li&gt;
&lt;li&gt;in the dialog, select “CD/DVD drive 1” in the device list&lt;/li&gt;
&lt;li&gt;select “connect at power on”&lt;/li&gt;
&lt;li&gt;choose “datastore ISO file” as the device type&lt;/li&gt;
&lt;li&gt;browse the datastore and select your config drive ISO&lt;/li&gt;
&lt;li&gt;confirm the changes and click “OK”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NOTE:如果发现ip不对，可查看配置文件，Maybe重启一下也可以解决哟。&lt;/p&gt;

&lt;h2 id=&#34;logging-in:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Logging in&lt;/h2&gt;

&lt;p&gt;可以查看ESXi控制台CoreOS的IP，但是静态的自己已经知道了。&lt;/p&gt;

&lt;p&gt;Now you can login using your SSH key or password set in your cloud-config，可以登录就没必要折腾下步了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh core@192.1.1.150
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, if the cloud-config fails to apply you can append coreos.autologin to the kernel parameters on boot, the console won’t prompt for a password. This is handy for debugging.&lt;/p&gt;

&lt;p&gt;When GNU GRUB appears at boot, make sure CoreOS default is selected and press e, then add coreos.autologin after &lt;code&gt;$linux_append&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Before&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.holysh1t.net/vgwtest/coreosstuff/grubautologin1.png&#34; alt=&#34;之前的启动界面&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.holysh1t.net/vgwtest/coreosstuff/grubautologin2.png&#34; alt=&#34;之前的启动界面&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When coreos.autologin is added, press &lt;code&gt;CTRL+X&lt;/code&gt; to boot CoreOS with these parameters. Note that the next time autologin will be disabled again as these kernel parameters aren’t persistent.&lt;/p&gt;

&lt;p&gt;You can now manually apply the cloud-config by using the following command in the console of CoreOS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo /usr/bin/coreos-cloudinit --from-file /media/configdrive/openstack/latest/user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;adding-new-machines:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Adding New Machines&lt;/h2&gt;

&lt;p&gt;按照前面所说的，如果需要把其他CoreOS加入集群，只需要把Discovery URL改成原来集群地址即可自动加入了，是不是很方便呀。&lt;/p&gt;

&lt;p&gt;If you forgot which discovery URL you used, you may look it up on one of the members of the cluster. Use the following grep command on one of your existing machines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep DISCOVERY /run/systemd/system/etcd2.service.d/20-cloudinit.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will see a line the contains the original discovery URL, like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Environment=&amp;quot;ETCD_DISCOVERY=https://discovery.etcd.io/575302f03f4fb2db82e81ea2abca55e9&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Your basic CoreOS cluster is set up, and now you can move on to testing with it!&lt;/p&gt;

&lt;h2 id=&#34;一些有用的coreos命令:8a4718f7d5df59499af8fd92000810ea&#34;&gt;一些有用的CoreOS命令&lt;/h2&gt;

&lt;p&gt;查看当前集群所有machines&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fleetctl list-machines
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看服务运行状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl -l status etcd2  #其中-l参数可选
systemctl -l status fleet
systemctl -l status docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看服务的运行日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -u etcd2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;参考资料:8a4718f7d5df59499af8fd92000810ea&#34;&gt;参考资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://coreos.com/&#34;&gt;CoreOS官网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ibm.com/developerworks/cn/cloud/library/1505_gutb_coreos/&#34;&gt;在 ESXi5 上部署 CoreOS 集群解决方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.tuicool.com/m/articles/zyaAbyJ&#34;&gt;平台云基石-CoreOS之集群篇&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stable.release.core-os.net/&#34;&gt;Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.holysh1t.net/vgwtest/coreosstuff/coreos-vmware-esxi-setup.html&#34;&gt;Running CoreOS on VMware ESXi 5.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Operation</title>
      <link>http://wixb50.github.io/2015/12/11/docker-operation/</link>
      <pubDate>Fri, 11 Dec 2015 18:38:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/12/11/docker-operation/</guid>
      <description>

&lt;h2 id=&#34;目录:9623c25974239aae48309dcafd31d278&#34;&gt;目录&lt;/h2&gt;

&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#normal-commander&#34;&gt;normal commander&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#docker-使用代理连接-docker-hub&#34;&gt;Docker 使用代理连接 Docker Hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#存出或者载入镜像&#34;&gt;存出或者载入镜像&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#批量操作docker-commander&#34;&gt;批量操作docker commander.&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#stop-all-containers&#34;&gt;Stop all containers.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remove-all-stopped-containers&#34;&gt;Remove all stopped containers.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remove-all-untagged-images&#34;&gt;Remove all untagged images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remove-all-images&#34;&gt;Remove all images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;h2 id=&#34;normal-commander:9623c25974239aae48309dcafd31d278&#34;&gt;normal commander&lt;/h2&gt;

&lt;h3 id=&#34;docker-使用代理连接-docker-hub:9623c25974239aae48309dcafd31d278&#34;&gt;Docker 使用代理连接 Docker Hub&lt;/h3&gt;

&lt;p&gt;如果你的宿主操作系统是 linux 那方法就很简单了，直接通过命令来启动服务即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo HTTP_PROXY=10.125.156.21:8118 docker -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是只是临时使用可以用下面语句&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo HTTP_PROXY=10.125.156.21:8118 docker pull node
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;存出或者载入镜像:9623c25974239aae48309dcafd31d278&#34;&gt;存出或者载入镜像&lt;/h3&gt;

&lt;p&gt;存出镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker save -o ubuntu_14.04.tar ubuntu:14.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;载入镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker load &amp;lt; ubuntu_14.04.tar
#or
sudo docker --input ubuntu_14.04.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;批量操作docker-commander:9623c25974239aae48309dcafd31d278&#34;&gt;批量操作docker commander.&lt;/h2&gt;

&lt;p&gt;NOTE: &lt;code&gt;sudo&lt;/code&gt;maybe.&lt;/p&gt;

&lt;h4 id=&#34;stop-all-containers:9623c25974239aae48309dcafd31d278&#34;&gt;Stop all containers.&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker stop $(docker ps -a -q)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;remove-all-stopped-containers:9623c25974239aae48309dcafd31d278&#34;&gt;Remove all stopped containers.&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker rm $(docker ps -a -q)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;remove-all-untagged-images:9623c25974239aae48309dcafd31d278&#34;&gt;Remove all untagged images&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker rmi $(docker images | grep &amp;quot;^&amp;lt;none&amp;gt;&amp;quot; | awk &amp;quot;{print $3}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;remove-all-images:9623c25974239aae48309dcafd31d278&#34;&gt;Remove all images&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker rmi $(docker images | grep \ | awk &#39;{print $3}&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>docker安装与配置</title>
      <link>http://wixb50.github.io/2015/11/02/docker%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Mon, 02 Nov 2015 21:38:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/11/02/docker%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</guid>
      <description>

&lt;h3 id=&#34;方法一-安装命令:a3a6e387cbc471e9a108d854770f0455&#34;&gt;方法一：安装命令&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install apt-transport-https
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9
sudo bash -c &amp;quot;echo deb https://get.docker.io/ubuntu docker main &amp;gt; /etc/apt/sources.list.d/docker.list&amp;quot;
sudo apt-get update
sudo apt-get install lxc-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;方法二-安装命令-推荐:a3a6e387cbc471e9a108d854770f0455&#34;&gt;方法二：安装命令(推荐)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完&lt;/p&gt;

&lt;h3 id=&#34;docker资料收集:a3a6e387cbc471e9a108d854770f0455&#34;&gt;Docker资料收集&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586#.74cyoyjp5&#34;&gt;Deploy a Mesos Cluster with 7 Commands Using Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://technologyconversations.com/2015/11/04/docker-clustering-tools-compared-kubernetes-vs-docker-swarm/&#34;&gt;Docker Clustering Tools Compared: Kubernetes vs Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mesosphere.com/blog/2014/11/10/docker-on-mesos-with-marathon/&#34;&gt;DOCKER CLUSTERING ON MESOS WITH MARATHON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.tuicool.com/articles/nyyENrY&#34;&gt;Swarm、Fleet、Kubernetes和Mesos的比较&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>