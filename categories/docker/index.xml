<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on Wixb blog</title>
    <link>http://wixb50.github.io/categories/docker/</link>
    <description>Recent content in Docker on Wixb blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>wixb50@gmail.com (Wixb)</managingEditor>
    <webMaster>wixb50@gmail.com (Wixb)</webMaster>
    <copyright>(c) 2015 wixb.All rights reserved.</copyright>
    <lastBuildDate>Fri, 17 Jun 2016 19:38:38 +0800</lastBuildDate>
    <atom:link href="http://wixb50.github.io/categories/docker/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>docker入门</title>
      <link>http://wixb50.github.io/2016/06/17/docker%E5%85%A5%E9%97%A8/</link>
      <pubDate>Fri, 17 Jun 2016 19:38:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2016/06/17/docker%E5%85%A5%E9%97%A8/</guid>
      <description>

&lt;h1 id=&#34;前言:11cc16163169598471edee7a60af0f73&#34;&gt;&lt;a href=&#34;#&#34;&gt;前言&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;虚拟化&lt;/strong&gt;，是指通过虚拟化技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt; 是个开源项目，它彻底释放了虚拟化的威力，极大提高了应用的运行效率，降低了云计算资源供应的成本，同时让应用的部署、测试和分发都变得前所未有的高效和轻松。Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。&lt;/p&gt;

&lt;p&gt;如果把虚拟化比做OS，则Docker是OS上的再一层抽象，它运行的容器可以看作一个进程级别的虚拟机。启动、运行、安装都是只通过一个命令就能解决。对于搭建集群和不可变基础设施是非常有利的。&lt;/p&gt;

&lt;h1 id=&#34;基本概念:11cc16163169598471edee7a60af0f73&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;Docker 包括三个基本概念&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;镜像（Image）&lt;/li&gt;
&lt;li&gt;容器（Container）&lt;/li&gt;
&lt;li&gt;仓库（Repository）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;理解了这三个概念，就理解了 Docker 的整个生命周期。&lt;/p&gt;

&lt;h2 id=&#34;镜像:11cc16163169598471edee7a60af0f73&#34;&gt;镜像&lt;/h2&gt;

&lt;p&gt;Docker 镜像（Image）就是一个只读的模板，相当于操作系统(OS)安装的Ghost。&lt;/p&gt;

&lt;p&gt;例如：一个镜像可以包含一个完整的 ubuntu 操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。&lt;/p&gt;

&lt;p&gt;镜像可以用来创建 Docker 容器。&lt;/p&gt;

&lt;p&gt;Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。&lt;/p&gt;

&lt;h2 id=&#34;容器:11cc16163169598471edee7a60af0f73&#34;&gt;容器&lt;/h2&gt;

&lt;p&gt;Docker 利用容器（Container）来运行应用。&lt;/p&gt;

&lt;p&gt;容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。同时容器也可以暴露相应的“端口”和挂载“容器卷”，分别用以网络和存储共享。&lt;/p&gt;

&lt;p&gt;可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。&lt;/p&gt;

&lt;h2 id=&#34;仓库:11cc16163169598471edee7a60af0f73&#34;&gt;仓库&lt;/h2&gt;

&lt;p&gt;仓库（Repository）是集中存放镜像文件的场所。&lt;/p&gt;

&lt;p&gt;仓库分为公开仓库（Public）和私有仓库（Private）两种形式。&lt;/p&gt;

&lt;p&gt;最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。&lt;/p&gt;

&lt;p&gt;国内的公开仓库包括 时速云 、网易云 等，可以提供大陆用户更稳定快速的访问。&lt;/p&gt;

&lt;p&gt;当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。&lt;/p&gt;

&lt;p&gt;*注：Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。&lt;/p&gt;

&lt;h2 id=&#34;安装:11cc16163169598471edee7a60af0f73&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;Docker 目前只能安装在 64 位平台上，并且要求内核版本不低于 3.10，实际上内核越新越好，过低的内核版本容易造成功能的不稳定。&lt;/p&gt;

&lt;p&gt;快捷安装脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;常用命令:11cc16163169598471edee7a60af0f73&#34;&gt;常用命令&lt;/h1&gt;

&lt;h2 id=&#34;镜像命令:11cc16163169598471edee7a60af0f73&#34;&gt;镜像命令&lt;/h2&gt;

&lt;p&gt;获取镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker pull ubuntu:12.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载过程中，会输出获取镜像的每一层信息。
该命令实际上相当于 $ sudo docker pull registry.hub.docker.com/ubuntu:12.04 命令，即从注册服务器 registry.hub.docker.com 中的 ubuntu 仓库来下载标记为 12.04 的镜像。&lt;/p&gt;

&lt;p&gt;使用 docker images 显示本地已有的镜像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker images
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;移除本地的镜像，可以使用 docker rmi 命令。注意 docker rm 命令是移除容器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker rmi training/sinatra
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;容器命令:11cc16163169598471edee7a60af0f73&#34;&gt;容器命令&lt;/h2&gt;

&lt;p&gt;基于一个images，新建并启动一个容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run ubuntu:14.04 /bin/echo &#39;Hello world&#39;
Hello world
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列出所有启动的容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列出所有的容器(包括未启动的)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker ps -a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据容器id，或者name启动，重启，停止容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker start/restart {containerID|containerName}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多的时候，需要让 Docker在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d ubuntu:14.04 /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot;
77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时容器会在后台运行并不会把输出的结果(STDOUT)打印到宿主机上面(输出结果可以用docker logs 查看)。&lt;/p&gt;

&lt;p&gt;容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p 参数来指定端口映射。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d -p 5000:5000 training/webapp python app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 hostPort:containerPort 格式本地的 5000 端口映射到容器的 5000 端口.&lt;/p&gt;

&lt;h1 id=&#34;docker进阶:11cc16163169598471edee7a60af0f73&#34;&gt;Docker进阶&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;数据卷管理&lt;/li&gt;
&lt;li&gt;Dockerfile&lt;/li&gt;
&lt;li&gt;私有仓库&lt;/li&gt;
&lt;li&gt;集群项目

&lt;ul&gt;
&lt;li&gt;Docker compose、Docker machine、Docker swarm&lt;/li&gt;
&lt;li&gt;Kubernetes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker集群系列之－Kubernetes对象文件定义</title>
      <link>http://wixb50.github.io/2016/03/13/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Bkubernetes%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6%E5%AE%9A%E4%B9%89/</link>
      <pubDate>Sun, 13 Mar 2016 15:10:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2016/03/13/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Bkubernetes%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6%E5%AE%9A%E4%B9%89/</guid>
      <description>

&lt;h2 id=&#34;说明&#34;&gt;说明&lt;/h2&gt;

&lt;p&gt;主要对kubernetes用户需要定义的Pod,RC和Service的配置文件进行详细说明。&lt;/p&gt;

&lt;h2 id=&#34;pod定义文件详解&#34;&gt;Pod定义文件详解&lt;/h2&gt;

&lt;p&gt;Pod定义文件模板(yaml格式)如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1      # required
kind: Pod           # required
metadata:           # required
  name: string      # required
  namespace: string      # required
  labels:
    - name: string
  annotations:
    - name: string
spec:       # required
  containers:       # required
    - name: string      # required
      image: string     # required
      imagePullPolicy: [Always | Never | IfNotPresent]
      command: [string]
      workingDir: string
      volumeMounts:
        - name: string
          mountPath: string
          readOnly: boolean
      ports:
        - name: string
          containerPort: int
          hostPort: int
          protocol: string
      env:
        - name: string
          value: string
      resources:
        limits:
         cpu: string
         memory: string
  volumes:
    - name: string
      # Either emptyDir for an empty directory
      emptyDir: {}
      # Or hostPath for a pre-existing directory on the host
      hostPath:
        path: string
  restartPolicy: [Always | Never | OnFailure]
  dnsPlicy: [Default | ClusterFirst]        # required
  nodeSelector: object
  imagePullSercret: object
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对Pod各属性详细说明表如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;属 性 名 称&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 类 型&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;是 否 必 选&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;version&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;v1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;kind&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Pod&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;元数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;rc定义文件详解&#34;&gt;RC定义文件详解&lt;/h2&gt;

&lt;p&gt;RC(ReplicationController)定义文件模板(yaml格式)如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1      # required
kind: ReplicationController           # required
metadata:           # required
  name: string      # required
  namespace: string      # required
  labels:
    - name: string
  annotations:
    - name: string
spec:       # required
  replicas: number      # required
  selector: []      # required
  template: object      # required
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;service定义文件详解&#34;&gt;Service定义文件详解&lt;/h2&gt;

&lt;p&gt;Service定义文件模板(yaml格式)如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1      # required
kind: Service           # required
metadata:           # required
  name: string      # required
  namespace: string      # required
  labels:
    - name: string
  annotations:
    - name: string
spec:       # required
  selector: []      # required
  type: string      #required
  clusterIP: string
  sessionAffinity: string
  ports:
    - name: string
      port: int
      targetPort: int
      protocol: string
  status:
    loadBalancer:
      ingress:
        ip: string
        hostname: string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对Service各属性详细说明表如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;属 性 名 称&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 类 型&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;是 否 必 选&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;取 值 说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;version&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;v1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;kind&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Pod&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;元数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.name&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Service名称，符合RFC1035规范&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.namespace&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;命名空间，不指定时系统使用名为&amp;raquo;default&amp;raquo;的命名空间&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.labels[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;自定义标签属性列表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;metadata.annotation[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;自定义注解属性列表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;详细描述&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.selector[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Label Selector配置，将选择具有指定label标签的Pod作为管理范围&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.type&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;string&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Required&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Service类型，指定Service访问方式，默认为ClusterIP。&lt;br&gt;ClusterIP: 虚拟的服务IP地址，该地址用户kubernetes集群内部Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发;&lt;br&gt;NodePort: 使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务;&lt;br&gt;LoadBalancer: 使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，并同时定义nodePort和clusterIP.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.clusterIP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;虚拟服务IP地址，当type=ClusterIP时，如果不指定，则系统自动分配;当type=LoadBalancer时，则需要指定。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.sessionAffinity&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是否支持Session，可选值为ClientIP，默认为空。&lt;br&gt;ClientIP: 表示同一个客户端(根据客户端的IP地址决定)的访问请求都转发到同一个后端Pod。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[]&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;list&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Service需要暴露的端口号列表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].name&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;String&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;端口名称&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].port&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;int&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;服务监听的端口号&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].targetPort&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;int&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;需要转发到后端Pod的端口号&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;spec.ports[].protocol&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;int&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;端口协议，支持TCP和UDP，默认TCP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;当spec.type=LoadBalancer时，设置外部负载均衡器的地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer.ingress&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer.ingress.ip&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;string&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器的IP地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;status.loadBalancer.ingress.hostname&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;string&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;外部负载均衡器的主机名&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;附录&#34;&gt;附录&lt;/h2&gt;

&lt;p&gt;根据yaml创建：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f &amp;lt;filename.yaml&amp;gt; [--validate[=true]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据yaml删除：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete -f &amp;lt;filename.yaml&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Docker集群系列之－ESXi5.5上搭建基于CoreOS的kubernetes集群</title>
      <link>http://wixb50.github.io/2015/12/30/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8Ecoreos%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Wed, 30 Dec 2015 19:10:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/12/30/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8Ecoreos%E7%9A%84kubernetes%E9%9B%86%E7%BE%A4/</guid>
      <description>

&lt;h1 id=&#34;目录:b041d08cd2cacb464d298a81037e9efc&#34;&gt;目录&lt;/h1&gt;

&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coreos-on-vmware&#34;&gt;CoreOS on VMware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloud-config-for-master-node&#34;&gt;Cloud-Config for master node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloud-config-for-minion-node&#34;&gt;Cloud-Config for minion node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#start-the-cluster&#34;&gt;Start the cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#check&#34;&gt;Check&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#enjoy&#34;&gt;Enjoy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;h1 id=&#34;introduction:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Create a Kubernetes Cluster on VMware ESXi with CoreOS.&lt;/p&gt;

&lt;h1 id=&#34;prerequisites:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Prerequisites&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;VMware ESXi

&lt;ul&gt;
&lt;li&gt;(optional) a DRS cluster with VCenter for high-availability host.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DHCP server&lt;/li&gt;
&lt;li&gt;A VMware datastore&lt;/li&gt;
&lt;li&gt;vSphere&lt;/li&gt;
&lt;li&gt;Attention: This requires at least CoreOS version 695.0.0, which includes etcd2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;coreos-on-vmware:b041d08cd2cacb464d298a81037e9efc&#34;&gt;CoreOS on VMware&lt;/h1&gt;

&lt;p&gt;Based on official documentation : &lt;a href=&#34;https://coreos.com/os/docs/latest/booting-on-vmware.html&#34;&gt;https://coreos.com/os/docs/latest/booting-on-vmware.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Download the OVA, on your local computer :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://alpha.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import ova on VMware via the vSphere Client :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in the menu, click &amp;quot;File &amp;gt; Deploy OVF Template...&amp;quot;
in the wizard, specify the location of the OVA downloaded earlier
name your VM
confirm the settings then click &amp;quot;Finish&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a template via vSphere Client :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;right click on the VM and Template &amp;gt; Convert into template
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can create -at least- 2 servers based on this template, do this task but don&amp;rsquo;t start it yet.&lt;/p&gt;

&lt;h1 id=&#34;cloud-config-for-master-node:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Cloud-Config for master node&lt;/h1&gt;

&lt;p&gt;On the VMware datastore, create a directory and initialize config, example :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p &amp;lt;path to datastore&amp;gt;/cloud-config/master/openstack/latest/ 
cd &amp;lt;path to datastore&amp;gt;/cloud-config/master/openstack/latest/
wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/getting-started-guides/coreos/cloud-configs/master.yaml &amp;amp;&amp;amp; mv master.yaml user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t forget to add your ssh_key :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim user_data
users:
  - name: &amp;quot;core&amp;quot;
    groups:
      - &amp;quot;sudo&amp;quot;
      - &amp;quot;docker&amp;quot;
    ssh-authorized-keys:   
      - ssh-rsa AAAA...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the master need a static ip address,so you need to add the ip config to &lt;code&gt;user_data&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim user_data
write-files:
  [...]
  - path: /etc/systemd/network/static.network
    permissions: 0644
    content: |
      [Match]
      Name=ens192  # The network card

      [Network]
      Address=192.1.1.150/24
      Gateway=192.1.1.1
      DNS=10.11.248.114
      DNS=8.8.4.4
  [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you can&amp;rsquo;t get the files on VM,you need download files first,and deploy to your &lt;code&gt;File Server&lt;/code&gt;first.And change the url in the &lt;code&gt;user_data&lt;/code&gt; to your own file position.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Replace `https://github.com/kelseyhightower/setup-network-environment/releases/download/v1.0.0/setup-network-environment` To `&amp;lt;your file url&amp;gt;/setup-network-environment`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-apiserver` To `&amp;lt;your file url&amp;gt;/kube-apiserver`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-controller-manager` To `&amp;lt;your file url&amp;gt;/kube-controller-manager`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-scheduler` To `&amp;lt;your file url&amp;gt;/kube-scheduler`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finaly, replace all &amp;laquo;$private_ipv4&amp;raquo; pattern with the ip of master node. The only way to perform this is to fix a DHCP lease with the MAC address of your master server. This MAC address can be get on vsphere : right click on VM, network adapter. Here, 10.0.0.1 is the master fixed ip address.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;s|$private_ipv4|10.0.0.1|g&#39; user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the limitation with coreOS and VMware : &lt;a href=&#34;https://coreos.com/os/docs/latest/booting-on-vmware.html#cloud-config&#34;&gt;https://coreos.com/os/docs/latest/booting-on-vmware.html#cloud-config&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Last step : create an iso :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd &amp;lt;path to datastore&amp;gt;/cloud-config/
mkisofs -R -V config-2 -o config-master.iso master/
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;cloud-config-for-minion-node:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Cloud-Config for minion node&lt;/h1&gt;

&lt;p&gt;On the VMware datastore, create a directory and initialize config, example :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p &amp;lt;path to datastore&amp;gt;/cloud-config/minion/openstack/latest/
cd &amp;lt;path to datastore&amp;gt;/cloud-config/minion/openstack/latest/
wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/docs/getting-started-guides/coreos/cloud-configs/node.yaml &amp;amp;&amp;amp; mv node.yaml user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t forget to add your ssh_key :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim user_data
users:
  - name: &amp;quot;core&amp;quot;
    groups:
      - &amp;quot;sudo&amp;quot;
      - &amp;quot;docker&amp;quot;
    ssh-authorized-keys:   
      - ssh-rsa AAAA...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you can&amp;rsquo;t get the files on VM,you need download files first,and deploy to your &lt;code&gt;File Server&lt;/code&gt;first.And change the url in the &lt;code&gt;user_data&lt;/code&gt; to your own file position.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Replace `https://github.com/kelseyhightower/setup-network-environment/releases/download/v1.0.0/setup-network-environment` To `&amp;lt;your file url&amp;gt;/setup-network-environment`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy` To `&amp;lt;your file url&amp;gt;/kube-proxy`
Replace `https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kubelet` To `&amp;lt;your file url&amp;gt;/kubelet`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finaly, replace all &amp;laquo;&lt;master-private-ip&gt;&amp;raquo; pattern with the ip of master node. (here 10.0.0.1)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;s|&amp;lt;master-private-ip&amp;gt;|10.0.0.1|g&#39; user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last step : create an iso :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd &amp;lt;path to datastore&amp;gt;/cloud-config/
mkisofs -R -V config-2 -o config-minion.iso minion/
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;start-the-cluster:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Start the cluster&lt;/h1&gt;

&lt;p&gt;On firt VM, mount the config-master.iso with VM properties (CD/DVD reader and &amp;laquo;Datastore ISO file&amp;raquo;), browse to &amp;laquo;&lt;path to datastore&gt;/cloud-config/&amp;laquo;. Don&amp;rsquo;t foget to set &amp;laquo;Connect on Start up&amp;raquo;.&lt;/p&gt;

&lt;p&gt;On second, and all other futher nodes  mount the config-minion.iso.&lt;/p&gt;

&lt;p&gt;Start your servers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;br /&gt;
In order to build the &lt;code&gt;flanneld&lt;/code&gt;,the VMs need to pull the images called &lt;code&gt;quay.io/coreos/flannel&lt;/code&gt;.And if the VMs can&amp;rsquo;t download it,you should get it first,then use the command to load the image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker load &amp;lt; flanneld-file.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ensure all the &lt;code&gt;service&lt;/code&gt; are running.&lt;br /&gt;
&lt;strong&gt;Master:&lt;/strong&gt;&lt;code&gt;docker&lt;/code&gt;、&lt;code&gt;etcd2&lt;/code&gt;、&lt;code&gt;fleet&lt;/code&gt;、&lt;code&gt;flanneld&lt;/code&gt;、&lt;code&gt;setup-network-environment&lt;/code&gt;、&lt;code&gt;kube-apiserver&lt;/code&gt;、&lt;code&gt;kube-controller-manager&lt;/code&gt;、&lt;code&gt;kube-scheduler&lt;/code&gt;.&lt;br /&gt;
&lt;strong&gt;Node:&lt;/strong&gt;&lt;code&gt;docker&lt;/code&gt;、&lt;code&gt;etcd2&lt;/code&gt;、&lt;code&gt;fleet&lt;/code&gt;、&lt;code&gt;flanneld&lt;/code&gt;、&lt;code&gt;setup-network-environment&lt;/code&gt;、&lt;code&gt;kubelet&lt;/code&gt;、&lt;code&gt;kube-proxy&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#you may use these command to start/enable your service:
sudo systemctl daemon-reload
sudo systemctl start &amp;lt;service-name&amp;gt;  #start the service
sudo systemctl enable &amp;lt;service-name&amp;gt;  #Ensure service can boot from the start
sudo systemctl status &amp;lt;service-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;check:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Check&lt;/h1&gt;

&lt;p&gt;Check your cluster heatlh : &lt;a href=&#34;http://10.0.0.1:8080/static/app/#/dashboard/&#34;&gt;http://10.0.0.1:8080/static/app/#/dashboard/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check each server :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh core@10.0.0.1
fleetctl list-machines
# and
journalctl -f
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;enjoy:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Enjoy&lt;/h1&gt;

&lt;p&gt;You may download the kubernetes client tool:&lt;code&gt;kubectl&lt;/code&gt;.Use it manage your cluster.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;get all minion node info.
&lt;code&gt;
kubectl get nodes
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;get all Pods.
&lt;code&gt;
kubectl get pods
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;get all Replication Controllers.
&lt;code&gt;
kubectl get rc
&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;get all Replication Services.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get svc
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;reference:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Reference&lt;/h1&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/xavierbaude/VMware-coreos-multi-nodes-Kubernetes&#34;&gt;VMware-coreos-multi-nodes-Kubernetes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000002886795&#34;&gt;kubernetes 0.18.1 安装 &amp;amp; 部署 &amp;amp; 初试&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services&#34;&gt;Installing Kubernetes Cluster with 3 minions on CentOS 7 to manage pods and services&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://dockerpool.com/article/1422538730&#34;&gt;如何在 CoreOS 集群上搭建 Kubernetes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://qiankunli.github.io/2015/01/29/Kubernetes_installation.html&#34;&gt;在CoreOS集群上搭建Kubernetes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://dockone.io/article/604&#34;&gt;CoreOS集成Kubernetes核心组件Kubelet&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;appendix:b041d08cd2cacb464d298a81037e9efc&#34;&gt;Appendix&lt;/h1&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://gist.github.com/anonymous/553e448c0f8ce9a23120&#34;&gt;source&lt;/a&gt; of &lt;code&gt;master.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://gist.github.com/anonymous/ce88bdc1f6368c0b1589&#34;&gt;source&lt;/a&gt; of &lt;code&gt;node.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker集群系列之－ESXi5.5上搭建CoreOS集群-01</title>
      <link>http://wixb50.github.io/2015/12/15/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BAcoreos%E9%9B%86%E7%BE%A4-01/</link>
      <pubDate>Tue, 15 Dec 2015 18:10:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/12/15/docker%E9%9B%86%E7%BE%A4%E7%B3%BB%E5%88%97%E4%B9%8Besxi5.5%E4%B8%8A%E6%90%AD%E5%BB%BAcoreos%E9%9B%86%E7%BE%A4-01/</guid>
      <description>

&lt;h2 id=&#34;目录:8a4718f7d5df59499af8fd92000810ea&#34;&gt;目录&lt;/h2&gt;

&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#null-link&#34;&gt;前言&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#前置条件&#34;&gt;前置条件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#安装coreos虚拟机&#34;&gt;安装CoreOS虚拟机&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-channel&#34;&gt;Choosing a Channel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deploying-with-vmware-vsphere-client-55&#34;&gt;Deploying with VMware vSphere Client 5.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloud-config&#34;&gt;Cloud-Config&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#需要说明的discovery&#34;&gt;需要说明的&lt;code&gt;discovery&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logging-in&#34;&gt;Logging in&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-new-machines&#34;&gt;Adding New Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#一些有用的coreos命令&#34;&gt;一些有用的CoreOS命令&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#参考资料&#34;&gt;参考资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;h2 id=&#34;前言-null-link:8a4718f7d5df59499af8fd92000810ea&#34;&gt;&lt;a href=&#34;chrome://not-a-link&#34;&gt;前言&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;因为大多数环境都是适配于大公司的云平台，但是也是有折中办法的。CoreOS是一个基于Linux 内核的轻量级操作系统，为了计算机集群的基础设施建设而生，专注于自动化，轻松部署，安全，可靠，规模化。作为一个操作系统，CoreOS 提供了在应用容器内部署应用所需要的基础功能环境以及一系列用于服务发现和配置共享的内建工具。而ESXi专为运行虚拟机、最大限度降低配置要求和简化部署而设计。所以我觉得使用ESXi当作IaaS架构，运行CoreOS集群，这样是可行的。话不多少，开始把。&lt;/p&gt;

&lt;h2 id=&#34;前置条件:8a4718f7d5df59499af8fd92000810ea&#34;&gt;前置条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;安装ESXi机器一台：怎么装自己应该知道把，如果因为驱动原因还需要自己定制安装ISO，见Google。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装coreos虚拟机:8a4718f7d5df59499af8fd92000810ea&#34;&gt;安装CoreOS虚拟机&lt;/h2&gt;

&lt;h3 id=&#34;choosing-a-channel:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Choosing a Channel&lt;/h3&gt;

&lt;p&gt;CoreOS is released into alpha, beta, and stable channels. Releases to each channel serve as a release-candidate for the next channel. For example, a bug-free alpha release is promoted bit-for-bit to the beta channel.&lt;/p&gt;

&lt;p&gt;The channel is selected based on the URLs below. Simply replace &lt;code&gt;stable&lt;/code&gt; with &lt;code&gt;alpha&lt;/code&gt; or &lt;code&gt;beta&lt;/code&gt; in the URL. Select 1 of these to download the appropriate image. Read the release notes for specific features and bug fixes in each channel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://stable.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://beta.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;curl -LO http://alpha.release.core-os.net/amd64-usr/current/coreos_production_vmware_ova.ova
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;deploying-with-vmware-vsphere-client-5-5:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Deploying with VMware vSphere Client 5.5&lt;/h3&gt;

&lt;p&gt;Use the vSphere Client to deploy the VM as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;in the menu, click “File &amp;gt; Deploy OVF Template…”&lt;/li&gt;
&lt;li&gt;in the wizard, specify the location of the OVA downloaded earlier&lt;/li&gt;
&lt;li&gt;name your VM&lt;/li&gt;
&lt;li&gt;choose “thin provision” for the disk format if you want the disk to grow dynamically&lt;/li&gt;
&lt;li&gt;choose your network settings&lt;/li&gt;
&lt;li&gt;confirm the settings then click “Finish”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NOTE: Unselect “Power on after deployment” so you have a chance to edit VM settings before powering it up for the first time.&lt;/p&gt;

&lt;p&gt;The last step uploads the files to your ESXi datastore and registers your VM. You can now tweak the VM settings, like memory and virtual cores. These instructions were tested to deploy to an ESXi 5.1 host.&lt;/p&gt;

&lt;p&gt;Before powering it on, you will have to create a cloud-config.&lt;/p&gt;

&lt;h2 id=&#34;cloud-config:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Cloud-Config&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/os/docs/latest/cloud-config.html&#34;&gt;Cloud-Config&lt;/a&gt;是CoreOS内比较重要的概念，可以理解为一种配置CoreOS的方式：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Providing Cloud-Config with Config-Drive&lt;/strong&gt;&lt;br /&gt;
Cloud-config can be specified by via &lt;a href=&#34;https://github.com/coreos/coreos-cloudinit/blob/master/Documentation/config-drive.md&#34;&gt;config-drive&lt;/a&gt; with the filesystem label &lt;code&gt;config-2&lt;/code&gt;. This is commonly done through whatever interface allows for attaching CD-ROMs or new drives.&lt;/p&gt;

&lt;p&gt;First create a user_data file using the the &lt;a href=&#34;https://coreos.com/os/docs/latest/cloud-config.html&#34;&gt;cloud-config guide&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config
hostname: core-01  #替换成你的命名的主机名
write_files:
    - path: /etc/systemd/network/static.network
      permissions: 0644  #文件权限,无需改
      content: |
        [Match]
        Name=ens192  #网卡名称,如果你的是别的名称,请改回来

        [Network]
        Address=192.1.1.150/24  #网络配置,同时把下面的IP改掉
        Gateway=192.1.1.1
        DNS=10.11.248.114
        DNS=8.8.4.4
coreos:
    etcd2:
        # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
        discovery: https://discovery.etcd.io/&amp;lt;token&amp;gt;  #这里在后面详细讲
        # multi-region and multi-cloud deployments need to use 192.1.1.150
        advertise-client-urls: http://192.1.1.150:2379
        initial-advertise-peer-urls: http://192.1.1.150:2380
        # listen on both the official ports and the legacy ports
        # legacy ports can be omitted if your application doesn&#39;t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://192.1.1.150:2380,http://192.1.1.150:7001
    fleet:
        public-ip: 192.1.1.150
        metadata: region=europe #metadata,可自定义
    flannel:
        etcd_prefix: /coreos.com/network2
    locksmith:
        endpoint: 192.1.1.150:4001
    update:
        reboot-strategy: etcd-lock
        group: stable
    units:
        - name: etcd2.service #注意是etcd2,第二版哟
          command: start
        - name: fleet.service
          command: start
users:
  - name: &amp;quot;core&amp;quot;  #改成你的用户名,可不是core
    groups:
      - &amp;quot;sudo&amp;quot;
      - &amp;quot;docker&amp;quot;
    ssh-authorized-keys:   
      - ssh-rsa 替换成你的公钥... 
manage_etc_hosts: localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cloud-Config配置信息验证地址&lt;a href=&#34;https://coreos.com/validate/&#34;&gt;https://coreos.com/validate/&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;需要说明的-discovery:8a4718f7d5df59499af8fd92000810ea&#34;&gt;需要说明的&lt;code&gt;discovery&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;　　因为要搭建集群，需要用到服务发现，配置集群的服务发现有两种方式：一种是Static方式，第二种就是Discovery方式了。其中个人不推荐第一种方式，因为每加入一台主机就需要手动配置etcd节点，非常不方便。&lt;br /&gt;
　　第二种Discovery方式是使用远程的服务器辅助服务发现，只需要配置好Discovery的URl就可以自动把新加入的服务器加入集群。其中iscovery服务器可以使用官网提供的，也可以自己搭建(我还没搭建过，这里不介绍了)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl https://discovery.etcd.io/new?size=3  #控制台或者浏览器执行即可,推荐使用size=1,见下面说明
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中有一个&lt;code&gt;size&lt;/code&gt;参数，讲一下我遇到的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;没有使用size参数结果老是启动不了;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;使用了&lt;code&gt;size=3&lt;/code&gt;，结果启动主节点，主节点的etcd2就一直等待从节点加入，结果等我去加入它的时候，已经超时了;&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;size=1&lt;/code&gt;，没有什么要等待了，过一会就自动启动成功了&lt;code&gt;fleetctl list-machines&lt;/code&gt;也能正常显示。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以，我可能不知道网上哪些一下子启动3个节点是怎么做到的，还是有待学习。但是这里我也有自己的解决方法，就是使用&lt;code&gt;size=1&lt;/code&gt;先运行出来一个只有一台主机的集群，果然可以运行。然后使用主节点的&lt;code&gt;&amp;lt;Token&amp;gt;&lt;/code&gt;再去构建其他节点的&lt;code&gt;Cloud-config&lt;/code&gt;，然后运行，结果果然它自己就能加入到第一个节点里面。&lt;/p&gt;

&lt;p&gt;这里我可能投机取巧了点，但是能运行，也能达到效果就行，哈哈，希望不会有什么bug。&lt;/p&gt;

&lt;p&gt;Finally, to create a cloud-config ISO, use the following commands using the user_data file we just created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#wrap up a config named user_data in a config drive image:
mkdir -p /tmp/new-drive/openstack/latest
cp user_data /tmp/new-drive/openstack/latest/user_data
mkisofs -R -V config-2 -o configdrive-01.iso /tmp/new-drive
rm -r /tmp/new-drive

#transform iso file to datastore
#scp configdrive-01.iso root@192.1.1.132:/vmfs/volumes/datastore1/ISO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the config-drive standard was originally an OpenStack feature, which is why you’ll see strings containing openstack. This filepath needs to be retained, although CoreOS supports config-drive on all platforms.&lt;/p&gt;

&lt;p&gt;Note: The $private_ipv4 and $public_ipv4 substitution variables referenced in other documents are not supported on VMware. You can replace all these variables by the (static) IP of the CoreOS server you’re setting up. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;coreos:
  etcd2:
    # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
    discovery: https://discovery.etcd.io/&amp;lt;token&amp;gt;
    # multi-region and multi-cloud deployments need to use $public_ipv4
    advertise-client-urls: http://$public_ipv4:2379
    initial-advertise-peer-urls: http://$private_ipv4:2380
    # listen on both the official ports and the legacy ports
    # legacy ports can be omitted if your application doesn&#39;t depend on them
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;becomes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;coreos:
  etcd2:
    # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
    discovery: https://discovery.etcd.io/&amp;lt;token&amp;gt;
    # multi-region and multi-cloud deployments need to use $public_ipv4
    advertise-client-urls: http://192.168.0.100:2379
    initial-advertise-peer-urls: http://192.168.0.100:2380
    # listen on both the official ports and the legacy ports
    # legacy ports can be omitted if your application doesn&#39;t depend on them
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://192.168.0.100:2380,http://192.168.0.100:7001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Attach the ISO to the VM as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;edit the settings of the CoreOS VM&lt;/li&gt;
&lt;li&gt;in the dialog, select “CD/DVD drive 1” in the device list&lt;/li&gt;
&lt;li&gt;select “connect at power on”&lt;/li&gt;
&lt;li&gt;choose “datastore ISO file” as the device type&lt;/li&gt;
&lt;li&gt;browse the datastore and select your config drive ISO&lt;/li&gt;
&lt;li&gt;confirm the changes and click “OK”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NOTE:如果发现ip不对，可查看配置文件，Maybe重启一下也可以解决哟。&lt;/p&gt;

&lt;h2 id=&#34;logging-in:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Logging in&lt;/h2&gt;

&lt;p&gt;可以查看ESXi控制台CoreOS的IP，但是静态的自己已经知道了。&lt;/p&gt;

&lt;p&gt;Now you can login using your SSH key or password set in your cloud-config，可以登录就没必要折腾下步了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh core@192.1.1.150
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, if the cloud-config fails to apply you can append coreos.autologin to the kernel parameters on boot, the console won’t prompt for a password. This is handy for debugging.&lt;/p&gt;

&lt;p&gt;When GNU GRUB appears at boot, make sure CoreOS default is selected and press e, then add coreos.autologin after &lt;code&gt;$linux_append&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Before&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.holysh1t.net/vgwtest/coreosstuff/grubautologin1.png&#34; alt=&#34;之前的启动界面&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.holysh1t.net/vgwtest/coreosstuff/grubautologin2.png&#34; alt=&#34;之前的启动界面&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When coreos.autologin is added, press &lt;code&gt;CTRL+X&lt;/code&gt; to boot CoreOS with these parameters. Note that the next time autologin will be disabled again as these kernel parameters aren’t persistent.&lt;/p&gt;

&lt;p&gt;You can now manually apply the cloud-config by using the following command in the console of CoreOS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo /usr/bin/coreos-cloudinit --from-file /media/configdrive/openstack/latest/user_data
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;adding-new-machines:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Adding New Machines&lt;/h2&gt;

&lt;p&gt;按照前面所说的，如果需要把其他CoreOS加入集群，只需要把Discovery URL改成原来集群地址即可自动加入了，是不是很方便呀。&lt;/p&gt;

&lt;p&gt;If you forgot which discovery URL you used, you may look it up on one of the members of the cluster. Use the following grep command on one of your existing machines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep DISCOVERY /run/systemd/system/etcd2.service.d/20-cloudinit.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will see a line the contains the original discovery URL, like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Environment=&amp;quot;ETCD_DISCOVERY=https://discovery.etcd.io/575302f03f4fb2db82e81ea2abca55e9&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion:8a4718f7d5df59499af8fd92000810ea&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Your basic CoreOS cluster is set up, and now you can move on to testing with it!&lt;/p&gt;

&lt;h2 id=&#34;一些有用的coreos命令:8a4718f7d5df59499af8fd92000810ea&#34;&gt;一些有用的CoreOS命令&lt;/h2&gt;

&lt;p&gt;查看当前集群所有machines&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fleetctl list-machines
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看服务运行状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl -l status etcd2  #其中-l参数可选
systemctl -l status fleet
systemctl -l status docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看服务的运行日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -u etcd2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;参考资料:8a4718f7d5df59499af8fd92000810ea&#34;&gt;参考资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://coreos.com/&#34;&gt;CoreOS官网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ibm.com/developerworks/cn/cloud/library/1505_gutb_coreos/&#34;&gt;在 ESXi5 上部署 CoreOS 集群解决方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.tuicool.com/m/articles/zyaAbyJ&#34;&gt;平台云基石-CoreOS之集群篇&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stable.release.core-os.net/&#34;&gt;Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.holysh1t.net/vgwtest/coreosstuff/coreos-vmware-esxi-setup.html&#34;&gt;Running CoreOS on VMware ESXi 5.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Operation</title>
      <link>http://wixb50.github.io/2015/12/11/docker-operation/</link>
      <pubDate>Fri, 11 Dec 2015 18:38:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/12/11/docker-operation/</guid>
      <description>

&lt;h2 id=&#34;目录:9623c25974239aae48309dcafd31d278&#34;&gt;目录&lt;/h2&gt;

&lt;!-- MarkdownTOC --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#normal-commander&#34;&gt;normal commander&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#docker-使用代理连接-docker-hub&#34;&gt;Docker 使用代理连接 Docker Hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#存出或者载入镜像&#34;&gt;存出或者载入镜像&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#批量操作docker-commander&#34;&gt;批量操作docker commander.&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#stop-all-containers&#34;&gt;Stop all containers.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remove-all-stopped-containers&#34;&gt;Remove all stopped containers.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remove-all-untagged-images&#34;&gt;Remove all untagged images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remove-all-images&#34;&gt;Remove all images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /MarkdownTOC --&gt;

&lt;h2 id=&#34;normal-commander:9623c25974239aae48309dcafd31d278&#34;&gt;normal commander&lt;/h2&gt;

&lt;h3 id=&#34;docker-使用代理连接-docker-hub:9623c25974239aae48309dcafd31d278&#34;&gt;Docker 使用代理连接 Docker Hub&lt;/h3&gt;

&lt;p&gt;如果你的宿主操作系统是 linux 那方法就很简单了，直接通过命令来启动服务即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo HTTP_PROXY=10.125.156.21:8118 docker -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是只是临时使用可以用下面语句&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo HTTP_PROXY=10.125.156.21:8118 docker pull node
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;存出或者载入镜像:9623c25974239aae48309dcafd31d278&#34;&gt;存出或者载入镜像&lt;/h3&gt;

&lt;p&gt;存出镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker save -o ubuntu_14.04.tar ubuntu:14.04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;载入镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker load &amp;lt; ubuntu_14.04.tar
#or
sudo docker --input ubuntu_14.04.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;批量操作docker-commander:9623c25974239aae48309dcafd31d278&#34;&gt;批量操作docker commander.&lt;/h2&gt;

&lt;p&gt;NOTE: &lt;code&gt;sudo&lt;/code&gt;maybe.&lt;/p&gt;

&lt;h4 id=&#34;stop-all-containers:9623c25974239aae48309dcafd31d278&#34;&gt;Stop all containers.&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker stop $(docker ps -a -q)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;remove-all-stopped-containers:9623c25974239aae48309dcafd31d278&#34;&gt;Remove all stopped containers.&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker rm $(docker ps -a -q)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;remove-all-untagged-images:9623c25974239aae48309dcafd31d278&#34;&gt;Remove all untagged images&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker rmi $(docker images | grep &amp;quot;^&amp;lt;none&amp;gt;&amp;quot; | awk &amp;quot;{print $3}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;remove-all-images:9623c25974239aae48309dcafd31d278&#34;&gt;Remove all images&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;docker rmi $(docker images | grep \ | awk &#39;{print $3}&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>docker安装与配置</title>
      <link>http://wixb50.github.io/2015/11/02/docker%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Mon, 02 Nov 2015 21:38:38 +0800</pubDate>
      <author>wixb50@gmail.com (Wixb)</author>
      <guid>http://wixb50.github.io/2015/11/02/docker%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</guid>
      <description>

&lt;h3 id=&#34;方法一-安装命令:a3a6e387cbc471e9a108d854770f0455&#34;&gt;方法一：安装命令&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install apt-transport-https
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9
sudo bash -c &amp;quot;echo deb https://get.docker.io/ubuntu docker main &amp;gt; /etc/apt/sources.list.d/docker.list&amp;quot;
sudo apt-get update
sudo apt-get install lxc-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;方法二-安装命令-推荐:a3a6e387cbc471e9a108d854770f0455&#34;&gt;方法二：安装命令(推荐)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完&lt;/p&gt;

&lt;h3 id=&#34;docker资料收集:a3a6e387cbc471e9a108d854770f0455&#34;&gt;Docker资料收集&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586#.74cyoyjp5&#34;&gt;Deploy a Mesos Cluster with 7 Commands Using Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://technologyconversations.com/2015/11/04/docker-clustering-tools-compared-kubernetes-vs-docker-swarm/&#34;&gt;Docker Clustering Tools Compared: Kubernetes vs Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mesosphere.com/blog/2014/11/10/docker-on-mesos-with-marathon/&#34;&gt;DOCKER CLUSTERING ON MESOS WITH MARATHON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.tuicool.com/articles/nyyENrY&#34;&gt;Swarm、Fleet、Kubernetes和Mesos的比较&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>